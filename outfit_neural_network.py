# -*- coding: utf-8 -*-
"""Outfit_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ibAyTSB0CpJ-65olvQxqv3QQo1hLzAJ
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import cv2
import random
from collections import Counter
from tensorflow.keras.models import load_model

import tensorflow as tf
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras import backend as K
from tensorflow.keras import losses
from tensorflow.keras.optimizers import Adam

import os
# %matplotlib inline

from google.colab import drive
from keras.datasets import fashion_mnist

# The original training was in google colab
drive.mount('/content/drive')

image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/datasets/pictures_outfit/bauman/bauman-yeallow.jpg')

plt.imshow(image)

path = "/content/drive/MyDrive/datasetTemp/"

originals = []
images_toget = []

mean = np.zeros((224, 224, 3))
number_ofim = 0

# extract the photos from google drive first folder
for filename in os.listdir(path + "original2/"):
    if (filename.endswith("png")):
        number_ofim += 1
        print(filename)
        original = cv2.imread(path + "original2/" + filename)
        original = cv2.resize(original, (224, 224))
        originals.append(original)

        mean[:, :, 0] = mean[:, :, 0] + original[:, :, 0]
        mean[:, :, 1] = mean[:, :, 1] + original[:, :, 1]
        mean[:, :, 2] = mean[:, :, 2] + original[:, :, 2]

# extract the photos from google drive first folder 2 
for filename in os.listdir(path + "original/"):
    if (filename.endswith("png")):
        number_ofim += 1
        print(filename)
        original = cv2.imread(path + "original/" + filename)
        original = cv2.resize(original, (224, 224))
        originals.append(original)

        mean[:, :, 0] = mean[:, :, 0] + original[:, :, 0]
        mean[:, :, 1] = mean[:, :, 1] + original[:, :, 1]
        mean[:, :, 2] = mean[:, :, 2] + original[:, :, 2]

arrDress = []
arrBody = []
for filename in os.listdir(path + "body2/"):
    if filename.endswith("png"):
        body = path + "body2/" + filename
        arrBody.append(body)

for filename in os.listdir(path + "dress2/"):
    if filename.endswith("png"):
        dress = path + "dress2/" + filename
        arrDress.append(dress)

for filename in os.listdir(path + "body/"):
    if filename.endswith("png"):
        body = path + "body/" + filename
        arrBody.append(body)

for filename in os.listdir(path + "dress/"):
    if filename.endswith("png"):
        dress = path + "dress/" + filename
        arrDress.append(dress)

# genrate the masks and 
for i in range(len(arrBody)):
    body = cv2.imread(arrBody[i], 0)
    dress = cv2.imread(arrDress[i], 0)
    dress[dress == 255] = 0
    dress[dress > 0] = 255
    dress = cv2.resize(dress, (224, 224))

    body[body == 255] = 0
    body[body > 0] = 255
    body = cv2.resize(body, (224, 224))

    skin = body - dress
    bg = (255 - body) / 255
    skin = (255 - skin) / 255
    dress = (255 - dress) / 255

    gt = np.zeros((224, 224, 3))
    gt[:, :, 0] = (1 - skin)
    gt[:, :, 1] = (1 - dress)
    gt[:, :, 2] = bg
    images_toget.append(gt)

mean = mean / number_ofim
print(number_ofim)
mean = mean.astype('int')

import pickle

# pixel mean array
pickle.dump(mean, open(path + "meanArrpixels.pkl", "wb"))


# training the unet model 
def get_unet():
    inputs = Input((None, None, 3))
    # Contracting part
    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(c1)

    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(c2)

    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(c3)

    c4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
    c4 = Conv2D(256, (3, 3), activation='relu', padding='same')(c4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(c4)

    # expansive path
    c5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
    c5 = Conv2D(512, (3, 3), activation='relu', padding='same')(c5)

    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5), c4], axis=3)
    c6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
    c6 = Conv2D(256, (3, 3), activation='relu', padding='same')(c6)

    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6), c3], axis=3)
    c7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
    c7 = Conv2D(128, (3, 3), activation='relu', padding='same')(c7)

    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7), c2], axis=3)
    c8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
    c8 = Conv2D(64, (3, 3), activation='relu', padding='same')(c8)

    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8), c1], axis=3)
    c9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
    c9 = Conv2D(32, (3, 3), activation='relu', padding='same')(c9)

    c10 = Conv2D(3, (1, 1), activation='sigmoid')(c9)

    model = Model(inputs=[inputs], outputs=[c10])

    model.compile(optimizer=Adam(lr=1e-3), loss=losses.binary_crossentropy, metrics=['accuracy'])

    return model


model = get_unet()
model.summary()

Xtrain = np.asarray(originals) - mean.reshape(-1, 224, 224, 3)
Xtest = np.asarray(images_toget).reshape(-1, 224, 224, 3)
print(Xtest.shape)

model = get_unet()
history = model.fit(Xtrain, Xtest, epochs=120)

model.summary()
model.evaluate(Xtrain, Xtest)

plt.figure(figsize=[10, 5])
plt.subplot(121)
plt.plot(history.history['accuracy'])
print(history.history.keys())
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Training Accuracy',
            'Validation Accuracy'])
plt.title('Accuracy Curves')

plt.subplot(122)
plt.plot(history.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Training Loss',
            'Validation Loss'])
plt.title('Loss Curves')
plt.show()

model.save("/content/drive/MyDrive/Colab Notebooks/datasets/" + "unet.h5")
